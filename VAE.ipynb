{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyahkMar1mZH"
      },
      "source": [
        "**Variational Auto-Encoders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnm4a_fZ1mZI",
        "outputId": "dc04b4f4-cf54-4c30-f14d-b8353e1dab7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-15 15:22:42.529634: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-15 15:22:42.620657: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-08-15 15:22:43.301527: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/mario/Documents/Projects/anaconda3/envs/tensorflow/lib/\n",
            "2024-08-15 15:22:43.301594: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/mario/Documents/Projects/anaconda3/envs/tensorflow/lib/\n",
            "2024-08-15 15:22:43.301603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Reshape, Conv2DTranspose, Add, Conv2D, MaxPool2D, Dense, Flatten, InputLayer, BatchNormalization, Input\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from scipy import *\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g6o7Gur1mZM"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TN321Aq1mZN"
      },
      "source": [
        "### Data Prep 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVn8A4rh1mZN",
        "outputId": "5a9b9df7-f6eb-44b8-ed09-2222ad9fa114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(70000, 28, 28, 1)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
        "mnist_digits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN2EZoaX1mZP",
        "outputId": "a233e551-cab2-49c0-c029-8130b5ca5f21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<TensorSliceDataset element_spec=TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None)>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(mnist_digits)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-vy2I351mZQ"
      },
      "outputs": [],
      "source": [
        "train_dataset = (\n",
        "    dataset\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .batch(128)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAzkH88M1mZR"
      },
      "source": [
        "### Data Prep 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooRLx5jp1mZS",
        "outputId": "4c438ef2-c456-4815-aaa3-d374e63615b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(70000, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\")\n",
        "\n",
        "print(mnist_digits.shape)\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation\n",
        "train_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        ")\n",
        "\n",
        "x_train = train_gen.flow(\n",
        "    mnist_digits,\n",
        "    shuffle=True,\n",
        "    batch_size=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWDGumuS1mZU"
      },
      "source": [
        "## Modeling VAES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMs6VKzu1mZV"
      },
      "outputs": [],
      "source": [
        "LATENT_DIM = 2\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oklmcFjF1mZX"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egbwjnw-1mZX"
      },
      "outputs": [],
      "source": [
        "class Sampling(Layer):\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean, logvar = inputs\n",
        "        return mean + tf.math.exp(0.5*logvar)*tf.random.normal(shape = tf.shape((BATCH_SIZE, LATENT_DIM)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo9JA5C81mZY"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzPSm1df1mZZ",
        "outputId": "6bd1a794-c8c6-4ed0-e2f1-a123380fa06c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"VAE-Encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 14, 14, 32)   320         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 7, 7, 64)     18496       ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 3136)         0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           50192       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            34          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 2)            34          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " sampling (Sampling)            (None, 2)            0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 69,076\n",
            "Trainable params: 69,076\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-15 15:22:46.291079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-08-15 15:22:46.296199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-08-15 15:22:46.296370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-08-15 15:22:46.296850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-15 15:22:46.297897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-08-15 15:22:46.298050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-08-15 15:22:46.298165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-08-15 15:22:46.799798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-08-15 15:22:46.799990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-08-15 15:22:46.800145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-08-15 15:22:46.800254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6127 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "def Encoder(latent_dim = LATENT_DIM):\n",
        "\n",
        "    inputs = Input(shape=(28, 28, 1))\n",
        "    x = Conv2D(32, (3, 3), strides = 2, padding=\"same\")(inputs)\n",
        "    x = Conv2D(64, (3, 3), strides  = 2, padding=\"same\")(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "\n",
        "    mean = Dense(LATENT_DIM,)(x)\n",
        "    log_var = Dense(LATENT_DIM, )(x)\n",
        "\n",
        "    z = Sampling()([mean, log_var])\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[z,mean,log_var], name='VAE-Encoder')\n",
        "    return model\n",
        "\n",
        "Encoder_model = Encoder()\n",
        "Encoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CswnJFVq1mZZ"
      },
      "source": [
        "#### Testing Variational Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmWISkZk1mZa",
        "outputId": "1dff8364-a991-4e83-95da-f7d94ed2f82f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "teste = x_train[0][0]\n",
        "plt.imshow(teste)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF4PlMYk1mZa",
        "outputId": "183a8288-a6fc-4571-df32-5f46ed91a80c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([1, 28, 28, 1])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "teste = tf.expand_dims(teste, axis = 0)\n",
        "teste.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uggBMb971mZb",
        "outputId": "cf603e92-4b7b-46d1-fbb5-a9def6748c08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-15 15:22:47.699422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
            "2024-08-15 15:22:48.216257: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
            "2024-08-15 15:22:48.216465: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
            "2024-08-15 15:22:48.216482: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:85] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
            "2024-08-15 15:22:48.216792: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
            "2024-08-15 15:22:48.216879: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: Failed to launch ptxas\n",
            "Relying on driver to perform ptx compilation. \n",
            "Modify $PATH to customize ptxas location.\n",
            "This message will be only logged once.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Z : [[ 1.98149    -0.06550196]]\n",
            "Mean : [[-0.05572234  0.0628576 ]]\n",
            "Log_Var : [[-0.08412249 -0.02237741]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-15 15:22:48.537383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
          ]
        }
      ],
      "source": [
        "encoded_image = Encoder_model.predict(teste)\n",
        "print('Z : ' + str(encoded_image[0]))\n",
        "print('Mean : ' + str(encoded_image[1]))\n",
        "print('Log_Var : ' + str(encoded_image[2]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHzSqSNF1mZc"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkJzaoJq1mZc",
        "outputId": "de5013ef-dd48-495c-bd04-bbf928bb9fec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"VAE-Decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3136)              9408      \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 14, 14, 64)       36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 32)       18464     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        289       \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 65,089\n",
            "Trainable params: 65,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def Decoder():\n",
        "\n",
        "    latent_inputs = Input(shape=(LATENT_DIM,))\n",
        "    x = Dense(7*7*64, activation='relu')(latent_inputs)\n",
        "    x = Reshape((7,7,64))(x)\n",
        "\n",
        "    x = Conv2DTranspose(64,(3,3), strides= 2, padding=\"same\")(x)\n",
        "    x = Conv2DTranspose(32,(3,3), strides= 2, padding=\"same\")(x)\n",
        "    x = Conv2DTranspose(1,(3,3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    model = Model(inputs=latent_inputs, outputs=x, name='VAE-Decoder')\n",
        "    return model\n",
        "\n",
        "Decoder_model = Decoder()\n",
        "Decoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cy-J3Cj1mZc"
      },
      "source": [
        "#### Testing Variational Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0oQeLVa1mZd",
        "outputId": "52c7b514-4ce9-42b9-dcaa-7a5d61ad2136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 257ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Zero Shot\n",
        "\n",
        "Dencoded_image = Decoder_model.predict(encoded_image[0])\n",
        "plt.imshow(Dencoded_image[0])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hWNX4K-1mZd"
      },
      "source": [
        "## VAE Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYYxRSdZ1mZe",
        "outputId": "859ba9b6-6789-4df0-da29-95c24219e677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"VAE\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " VAE-Encoder (Functional)    [(None, 2),               69076     \n",
            "                              (None, 2),                         \n",
            "                              (None, 2)]                         \n",
            "                                                                 \n",
            " VAE-Decoder (Functional)    (None, 28, 28, 1)         65089     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134,165\n",
            "Trainable params: 134,165\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def VAE():\n",
        "    vae_inputs = Input(shape=(28, 28, 1))\n",
        "    z, _, _ = Encoder_model(vae_inputs)\n",
        "    outputs = Decoder_model(z)\n",
        "\n",
        "    vae_model = Model(vae_inputs, outputs, name='VAE')\n",
        "    return vae_model\n",
        "\n",
        "VAE_model = VAE()\n",
        "VAE_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOp5rUdj1mZe"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytAWZHVE1mZe"
      },
      "outputs": [],
      "source": [
        "def customized_loss(y_true, y_pred, mean, log_var):\n",
        "    # Loss que tenta aproximar os valores de cada pixel do correto\n",
        "    reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.binary_crossentropy(y_true, y_pred), axis=(1,2)))\n",
        "\n",
        "    # Loss que tenta aproximar os valores da média e do desvio padrão dos pixels\n",
        "    # Para isso, utiliza o logaritmo natural para evitar problemas com números negativos\n",
        "    regression_loss = tf.reduce_mean(tf.reduce_sum(-0.5*(log_var+1-tf.square(mean)-tf.exp(log_var)),axis = 1))\n",
        "\n",
        "    return reconstruction_loss + regression_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE4u4umj1mZf"
      },
      "outputs": [],
      "source": [
        "def training_block(x_batch):\n",
        "    with tf.GradientTape() as recorder:\n",
        "        z, mean, log_var = Encoder_model(x_batch)\n",
        "        y_pred = Decoder_model(z)\n",
        "        loss = customized_loss(x_batch, y_pred, mean, log_var)\n",
        "\n",
        "    gradients = recorder.gradient(loss, vae_model.trainable_weights)\n",
        "    Adam(learning_rate = 1e-3).apply_gradients(zip(gradients, vae_model.trainable_weights))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwUpn3Xc1mZf"
      },
      "outputs": [],
      "source": [
        "def train(data, epochs):\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        print('Epoch: ',epoch+1)\n",
        "\n",
        "        for step, x_batch in enumerate(data):\n",
        "            loss = training_block(x_batch)\n",
        "        print('Training Loss: ',loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGQEtwXJ1mZf",
        "outputId": "631d97da-76c9-4aa0-a3b9-105241c11c99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  1\n"
          ]
        }
      ],
      "source": [
        "train(train_dataset, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "ArQkkmct17ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scale=1\n",
        "n=16"
      ],
      "metadata": {
        "id": "Zf3dv3sA19UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_x = np.linspace(-scale,scale,16)\n",
        "grid_y = np.linspace(-scale,scale,16)"
      ],
      "metadata": {
        "id": "SkW9Sy4i1__H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,12))\n",
        "k=0\n",
        "for i in grid_x:\n",
        "  for j in grid_y:\n",
        "    ax=plt.subplot(n,n,k+1)\n",
        "\n",
        "    input=tf.constant([[i,j]])\n",
        "    out=VAE_model.layers[2].predict(input)[0][...,0]\n",
        "    plt.imshow(out,cmap=\"Greys_r\")\n",
        "    plt.axis('off')\n",
        "    k+=1"
      ],
      "metadata": {
        "id": "xf6Hjh302CaK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}